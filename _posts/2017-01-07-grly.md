---
layout: post
title: Grammarly Pings Data
tags: []
---

## Contents
---

* [Setting up](#setting-up)
* [Retention Curve](#retention-curve)
* [Analyzing the data](#analyzing-the-data)
* [Recommendations For Grammarly](#recommendations-for-grammarly)


## Setting up
---

I will be using R to analyze the pings data. The following packages are loaded :

* _jsonlite_   : To read in the json data from the file
* _lubridate_  : To parse the date time.
* _data.table_ : I prefer to use data.table over data.frame for performance efficiency.
* _ggplot2_    : To visualize data
* _dplyr_      : To perform data operations
* _nortest_    : To test normality of data
* _PMCMR_      : To run post HOC analysis on Kruskal's test results

~~~ r
> if (!require("pacman")) install.packages("pacman")
Loading required package: pacman
> pacman::p_load(data.table, ggplot2, dplyr, jsonlite, lubridate, nortest, PMCMR)
~~~

I read in the data and set up *attributed_to* and *first_utm_source* as factor columns, parse the *date_time* column and remove *fingerprint* and *user_id* columns.
I also add a date column, which has no information about the time of the ping. This makes it easier to group/filter the rows just by the date.

~~~ r
> grammarly <- stream_in(file("~/Projects/grammarly/data/pings.txt"))
opening file input connection.
 Imported 4850111 records. Simplifying...
closing file input connection.
> grly_table <- data.table(grammarly)
> grly_table$attributed_to <- as.factor(grly_table$attributed_to)
> grly_table$first_utm_source <- as.factor(grly_table$first_utm_source)
> grly_table$fingerprint <- NULL
> grly_table$user_id <- NULL
> grly_table$date_time <- parse_date_time(grly_table$date_time, orders = "ymd HM")
> grly_table$date <- as.Date(grly_table$date_time)
> summary(grly_table)
       attributed_to       date_time                   used_first_time_today first_utm_source        date
 u:-476374517 :   2853   Min.   :2016-02-01 00:00:00   Mode :logical         google  :1759351   Min.   :2016-02-01
 u:1425491950 :   2823   1st Qu.:2016-02-08 14:32:00   FALSE:4604611         brand1  : 400310   1st Qu.:2016-02-08
 f:1923316387 :   2794   Median :2016-02-16 02:36:00   TRUE :245500          youtube : 231929   Median :2016-02-16
 f:-274856460 :   2612   Mean   :2016-02-15 19:20:49   NA's :0               facebook: 194759   Mean   :2016-02-15
 u:1454129573 :   2194   3rd Qu.:2016-02-23 05:25:00                         taboola : 124552   3rd Qu.:2016-02-23
 u:-1055316730:   2192   Max.   :2016-02-29 23:59:00                         (Other) : 398527   Max.   :2016-02-29
 (Other)      :4834643                                                       NA's    :1740683   NA's    :1740683
~~~

Checking for duplicate rows in the data reveals that there some repeated rows. I remove them.

~~~ r
> sum(duplicated(grly_table))
[1] 732783
> grly_table <-  grly_table[!(duplicated(grly_table)),]
>
~~~

Now, the rows should be unique for each *attributed_to* id and *date-time*. But this wasn't true for few rows. On exploring that data
I noticed that it was the sources that was different. In all of these rows, the source for a given user id and time was either set
to a source or empty(NA). I think it is safe to remove the rows with empty source values.

~~~ r
> duplicate_rows <- grly_table%>% group_by(attributed_to, date_time) %>% filter(n() > 1)
>
> duplicate_rows
Source: local data frame [56 x 5]
Groups: attributed_to, date_time [28]

   attributed_to           date_time used_first_time_today first_utm_source       date
          <fctr>              <dttm>                 <lgl>           <fctr>     <date>
1   u:2119542499 2016-02-01 13:19:00                 FALSE               NA 2016-02-01
2   u:2119542499 2016-02-01 13:19:00                 FALSE           brand1 2016-02-01
3   u:-913070131 2016-02-03 00:53:00                 FALSE               NA 2016-02-03
4   u:-913070131 2016-02-03 00:53:00                 FALSE           brand1 2016-02-03
5   u:-207622223 2016-02-03 03:29:00                 FALSE               NA 2016-02-03
6   u:-207622223 2016-02-03 03:29:00                 FALSE           brand1 2016-02-03
7   u:-488024152 2016-02-03 11:36:00                 FALSE               NA 2016-02-03
8   u:-488024152 2016-02-03 11:36:00                 FALSE           google 2016-02-03
9   u:1712503559 2016-02-03 19:00:00                 FALSE               NA 2016-02-03
10  u:1712503559 2016-02-03 19:00:00                 FALSE           google 2016-02-03
# ... with 46 more rows
> grly_table <- grly_table %>% group_by(attributed_to, date_time) %>% filter(!(n() > 1 & is.na(first_utm_source)))
>
~~~

As an added step of verification, I make sure that no user has more than 1 source assigned to him/her.

~~~ r
> nrow(grly_table %>% group_by(attributed_to) %>% filter(!(is.na(first_utm_source))) %>% filter(length(unique(first_utm_source)) >1))
[1] 0
~~~

## Retention Curve
---

Using ggplot to plot the daily retention curve for users who used the app for the first time on Feb 4th, and Feb 10th.

~~~ r

> fourth <- as.Date("2016-02-04")
> tenth <- as.Date("2016-02-10")

> new_users_of_4th <- grly_table %>%
                        filter(date == fourth & used_first_time_today == TRUE)  %>%
                        select(attributed_to)
> new_users_of_10th <- grly_table %>%
                          filter(date == tenth & used_first_time_today == TRUE)  %>%
                          select(attributed_to)
> new_users_of_4th <- unique(new_users_of_4th$attributed_to)
> new_users_of_10th <- unique(new_users_of_10th$attributed_to)

> retention_data <- grly_table %>%
                      group_by(date)
                      %>% summarise(retention_rate_for_4th = length(intersect(new_users_of_4th,unique(attributed_to))) / length(new_users_of_4th),
                                    retention_rate_for_10th = length(intersect(new_users_of_10th,unique(attributed_to))) / length(new_users_of_10th))
> melted_retention_data <- melt(retention_data, id="date")
> ggplot(data = melted_retention_data, aes(x=date,y= value * 100, colour = variable))
    + geom_line()
    + labs(x = "Date (2016)", y = "Retention in %")
    + ggtitle("Daily Retention Curve for Cohorts of February 4th and 10th, 2016")
    + theme(plot.title = element_text(face = "bold"))
    + scale_colour_discrete(name="Cohorts", labels = c("4th February 2016", "10th February 2016"))
    + scale_y_discrete(limits = seq(from=0, to=100, by = 10))
>
~~~

![Daily Retention Curve](/data-analysis/assets/grly-retention-curve.png)

## Analyzing the data
---

In order to define the best and the worst users, I collect information about the number of pings, number of days logged in and the average pings per day for each user.

~~~ r
users_info <- grly_table %>%
                group_by(attributed_to) %>%
                arrange(date) %>% slice(1) %>%
                ungroup()
pings_info <- grly_table %>%
                group_by(attributed_to) %>%
                summarise(num_of_pings = length(unique(date_time)), num_of_days = length(unique(date)))
pings_info$avg_pings_per_day = pings_info$num_of_pings/pings_info$num_of_days
users<-merge(users_info,pings_info,by = "attributed_to",all = TRUE)
users$avg_pings_per_day <- round(users$avg_pings_per_day, digits = 2
)
~~~

I now plot the total number of pings against the total number of days for each user. The users are colored by the source.

~~~ r
ggplot(data = users, aes(x=num_of_days, y=num_of_pings))
  + geom_point(aes(colour = first_utm_source))
  + geom_smooth(method=lm)
  + theme(legend.position="bottom")
  + labs(x = "Total Number of Days logged in", y = "Total Number of Pings")
  + ggtitle("Total Number of days vs Total Number of Pings")
  + theme(plot.title = element_text(face = "bold"))
  + scale_colour_discrete(name="Source")
~~~

![Pings vs Days](/data-analysis/assets/grly-pings-vs-days.png)


From the plot above, it is clear that our best users lay at the extreme right of our plot and those at the top right use the app the most.
While those at the extreme left are our worst users. Coloring the users by their sources doesn't give much information as the sources seem to be widely distributed.
The regression line indicates there is a slight linear relationship between the number of days logged in and the number of pings. This shouldn't be much surprising.

The average pings per day, by itself, is not enough to indicate our best and worst users. As can be seen from the plot, some of the users on the left of the plot have high number of pings.
But if the user has only used the app for, say, 10 days with high number of pings each day and hasn't used since, then we have probably lost the interest of the user.

There are possibly many ways to combine the number of days and pings to get a measure of active users. For the purposes of this assignment, I will calculate the visit rate for each of the user, which is the ratio of the number of days they have been active over the total number of days since they joined in February 2016. For those users who do not have a *used_first_time_today*, I assume that they have joined before
the month of February (hence total number of days since they joined is 29).

~~~ r
> users$days_since_joined <- ifelse((users$used_first_time_today == TRUE), (as.Date("2016-03-01") - users$date ),29)
> users$visit_rate <- users$num_of_days / users$days_since_joined
> summary(users)
       attributed_to      date_time                   used_first_time_today  first_utm_source
 f:-1000067921:     1   Min.   :2016-02-01 00:00:00   Mode :logical         google   : 83365
 f:-100011650 :     1   1st Qu.:2016-02-01 09:44:00   FALSE:159991          brand1   : 14626
 f:-100013245 :     1   Median :2016-02-02 11:05:00   TRUE :89068           youtube  : 14408
 f:-1000233833:     1   Mean   :2016-02-07 20:31:35   NA's :0               facebook :  6679
 f:-1000235328:     1   3rd Qu.:2016-02-13 09:28:00                         placement:  6027
 f:-1000239650:     1   Max.   :2016-02-29 23:59:00                         (Other)  : 16325
 (Other)      :249053                                                       NA's     :107629
      date             num_of_pings      num_of_days   avg_pings_per_day days_since_joined   visit_rate
 Min.   :2016-02-01   Min.   :   1.00   Min.   : 1.0   Min.   :  1.000   Min.   : 1.00     Min.   :0.03448
 1st Qu.:2016-02-01   1st Qu.:   5.00   1st Qu.: 4.0   1st Qu.:  1.000   1st Qu.:20.00     1st Qu.:0.30000
 Median :2016-02-02   Median :  15.00   Median :14.0   Median :  1.070   Median :29.00     Median :0.65517
 Mean   :2016-02-07   Mean   :  16.53   Mean   :13.6   Mean   :  1.287   Mean   :23.75     Mean   :0.58001
 3rd Qu.:2016-02-13   3rd Qu.:  25.00   3rd Qu.:22.0   3rd Qu.:  1.270   3rd Qu.:29.00     3rd Qu.:0.86207
 Max.   :2016-02-29   Max.   :1137.00   Max.   :29.0   Max.   :115.000   Max.   :29.00     Max.   :1.00000
~~~

Plotting the histogram of the visit rate of the users shows a nice spread of the data. The distribution is not uniform. There are two peaks at the
extreme ends of the distribution. Some of the users with 100 % or a very low visit rate might be users who joined late in February and we don't have
enough information to analyze them.

~~~ r
> ggplot(users, aes(x=visit_rate))
  + geom_histogram(bins = 30, colour= "black", fill = "red", alpha=0.4)
  + labs(x = "Visit Rate", y = "Count")
  + ggtitle("Histogram of Visit Rate of Users")
  + theme(plot.title = element_text(face = "bold"))
~~~

![Histogram of Visit Rate](/data-analysis/assets/grly-histogram-visit-rate.png)


Before analyzing the data, I remove users who joined the app recently since we do not have enough information about them.

~~~ r
> older_users <- users %>% filter(date < as.Date("2016-02-27"))
~~~

I group the data by source and calculate the total number of users and their mean visit rate within each group.
I notice that some of the sources have users in  thousands and hundreds while some sources have less than 15 users.
For the purpose of this analysis, I only consider sources with more than 25 users.

~~~ r
> visit_rate_by_source <- older_users %>%
                          filter(!is.na(first_utm_source)) %>%
                          group_by(first_utm_source) %>%
                          summary(mean(older_users$daily_visit_rate))

> major_sources <- visit_rate_by_source %>%
                    filter(num_of_users>=25) %>%
                    select(first_utm_source)

> older_users_filtered_by_major_sources <- older_users %>%
                                              filter(first_utm_source %in% major_sources$first_utm_source)
> older_users_filtered_by_major_sources$first_utm_source <- droplevels(older_users_filtered_by_major_sources$first_utm_source)
~~~

I now plot the boxplot for the visit rate grouped by the source.

~~~ r
boxplot(older_users_filtered_by_major_sources$visit_rate~older_users_filtered_by_major_sources$first_utm_source,
          col=rainbow(21), las=2, horizontal = TRUE,
          xlab="Mean Visit Rate",
          main="Average Visit Rate for Each Source")
~~~

![Visit Rate by Source](/data-analysis/assets/grly-visit-rate-by-source.png)

Liveintent, Facebook_org., digg have a higher mean visit rate compared to others whereas Blog_org has the lowest. But given their difference in
sample sizes we would need to perform a statistical test to find out if the difference in means between these groups are significant enough to warrant
a fair comparison given their sample sizes.

Since the distribution is not normal and the sample sizes for each group are different, I opt for the Kruskal-Wallais test.

~~~ r
> kruskal.test(visit_rate~first_utm_source, data = older_users_filtered_by_major_sources)

	Kruskal-Wallis rank sum test

data:  visit_rate by first_utm_source
Kruskal-Wallis chi-squared = 737.7, df = 24, p-value < 2.2e-16

>
~~~

The significant value of p (less than 0.05) suggests that there is difference in the means between the sources.

I now use the Dunn tests (because the sample sizes are different) for post HOC analysis of the Kruskal test and print
the pairwise comparison of sources whose p-value are significant enough to suggest a difference in their means.

~~~ r
> phoc_dunn_results <- posthoc.kruskal.dunn.test(older_users_filtered_by_major_sources$daily_visit_rate~older_users_filtered_by_major_sources$first_utm_source)
Warning message:
In posthoc.kruskal.dunn.test.default(c(0.448275862068966, 0.551724137931034,  :
  Ties are present. z-quantiles were corrected for ties.
> phoc_dunn_matrix <- phoc_dunn_results$p.value
> for(rname in rownames(phoc_dunn_matrix)) {
+     for(cname in colnames(phoc_dunn_matrix)) {
+         if((!is.na(phoc_dunn_matrix[rname,cname])) & (phoc_dunn_matrix[rname,cname] < 0.05)) {
+             print (paste0(rname, " - ", cname, " => " , phoc_dunn_matrix[rname, cname]))
+         }
+     }
+ }
[1] "brand1 - bing => 0.000210738446987167"
[1] "facebook - bing => 8.76542463598507e-43"
[1] "facebook - brand1 => 4.92023900593908e-36"
[1] "Facebook_org - bing => 2.91342884000007e-09"
[1] "Facebook_org - brand1 => 9.90347751653531e-06"
[1] "Facebook_org - display => 0.0134332958680668"
[1] "google - brand1 => 3.35699573382709e-05"
[1] "google - facebook => 2.06547863296876e-75"
[1] "google - Facebook_org => 5.82252170738408e-08"
[1] "liveintent - bing => 2.98636126167868e-12"
[1] "liveintent - brand1 => 5.38119185001052e-08"
[1] "liveintent - display => 0.00139786967254759"
[1] "liveintent - google => 7.97089836755814e-11"
[1] "outbrain - bing => 0.0218822002327082"
[1] "outbrain - facebook => 0.0014261566822866"
[1] "outbrain - Facebook_org => 0.00517745253125689"
[1] "outbrain - liveintent => 0.000267245825281571"
[1] "pandora - bing => 5.41928564810506e-06"
[1] "pandora - facebook => 2.08677473319751e-06"
[1] "pandora - Facebook_org => 0.00341748377706667"
[1] "pandora - google => 0.000115975288888124"
[1] "pandora - liveintent => 0.000115292216015417"
[1] "placement - bing => 8.47595442410964e-11"
[1] "placement - brand1 => 0.020265947296985"
[1] "placement - facebook => 4.96546216880063e-11"
[1] "placement - Facebook_org => 0.0025439355309299"
[1] "placement - google => 2.69823539327442e-13"
[1] "placement - liveintent => 6.46496045315697e-05"
[1] "program - facebook => 9.33028677240994e-10"
[1] "program - Facebook_org => 9.91699870220079e-06"
[1] "program - liveintent => 1.44586701349269e-07"
[1] "taboola - bing => 8.32021415841435e-21"
[1] "taboola - brand1 => 3.06870661573022e-11"
[1] "taboola - facebook => 0.0397072658373553"
[1] "taboola - google => 1.31504591278828e-28"
[1] "taboola - liveintent => 0.0134959862745661"
[1] "taboola - program => 0.000618692149299563"
[1] "twitter - liveintent => 0.0356978057438594"
[1] "youtube - bing => 2.42594833192394e-17"
[1] "youtube - brand1 => 1.0735833836362e-07"
[1] "youtube - facebook => 6.88086639146184e-13"
[1] "youtube - Facebook_org => 0.0053025523364589"
[1] "youtube - google => 1.75912060317518e-37"
[1] "youtube - liveintent => 0.000149202546194226"
>
~~~
From the above pairwise comparisons the sources like facebook and  liveintent show a significant difference in their average visit rate compared to brand1, bing and program.
This suggests that the sources brand1, bing and program are not doing as well as others. Taboola, Placement, Pandora and Youtube seem to be doing reasonably well. Google, which brings in
the largest number of users, seems to be doing about average. If Grammarly is spending more of its ad budget with Google then it might be worth considering moving some of the money
to high performing sources like Facebook and Liveintent.


### Notes

* Some of the sources could be considered the one and the same. For example : bing and Bing, brand1 and [brand1,+brand1], facebook and Facebook_org and facebook.com, etc. I considered these sources to be separate since that might be the business intent. In some cases, the sample size are so small (Bing only had 4 users) that
it was safe to separate them. Given more time, I would have checked with the Product team to verify if these sources should indeed be considered as one and the same.

* The post HOC analysis showed a warning message that "ties are present" as show in the code above. There are tie-breaking methods that can be applied to handle this warning.

* I noticed that one of the users had more than one date as the first usage date.

~~~ r
> duplicate_first_date <- (grly_table %>% group_by(attributed_to) %>% filter(used_first_time_today == TRUE) %>% filter(length(unique(date)) >1))
> duplicate_first_date
Source: local data frame [2 x 5]
Groups: attributed_to [1]

  attributed_to           date_time used_first_time_today first_utm_source       date
         <fctr>              <dttm>                 <lgl>           <fctr>     <date>
1  f:1756251777 2016-02-18 09:49:00                  TRUE               NA 2016-02-18
2  f:1756251777 2016-02-16 00:46:00                  TRUE               NA 2016-02-16
~~~
When filtering the data I used the earliest date. Since it is only one user I don't think it affects our analysis.


## Recommendations for Grammarly
---

### Data-related Projects

* Assuming we don't violate privacy issues the following features might be useful:

  - If you can keep a history of the grammatical and spelling mistakes a user makes then it would be nice to send out a test, say at the end of each month, to help
the users avoid making them in future. We can also send a more general test for all the users based on the common mistakes made by the users of Grammarly.
Also, when the user makes an error when they are typing, we could show a statistic like how many other users (or maybe the percentile of users who) make the same mistake.
This could be useful feedback to the user.

  - If we can detect the typing speed of the users and if we can establish a relationship between the numbers of errors they make with the typing speed, then we can make a suggestion
to either lower or increase the typing speed.

  - We can analyze the data typed to see if certain words shows an increase in usage at a certain point in time and if so identify what caused it like political events, pop culture, etc.
I believe the Oxford Dictionary releases new words every month/year. It would be interesting to see how many of them have been used since they first originated.

* Instead of a fixed pricing, maybe Grammarly can consider a usage based charge. Based on the usage pattern of users, we can help come up with a pricing point that might attract users who have left or are planning to leave.

* USA (and/or UK) gets a lot of immigrants and non-immigrants (for work/education) from different countries. If we can analyze that data and can get a list of countries, we can target users originating from those countries through advertising. We can also get data on the different countries USA and UK outsource their work to. Since the correspondence between the workers of these countries will likely be in US or UK English, they would find Grammarly useful to correct their email, documents, etc. when they communicate with the parent company.


### Product Features
* I noticed that Grammarly provides vocabulary suggestions. I think it would be useful to help users build their vocabulary and learn new words.
And one of the best ways to do that would be to provide an etymology of the words. A service like http://www.etymonline.com/ would be helpful to have
at the click of a button. When the user uses a word that has an interesting history associated with its origin or when the user looks up the meaning of a word, then adding a
clean simple pop-up of the etymology of the word would add a pleasurable experience to the process of writing.

* An Alexa app, that will help users correct their sentence. That will also give an idea about which types of sentences confuse the users the most and how Grammarly can help them.

